{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "no_of_cpu = 8\n",
    "max_cores = 16\n",
    "executor_mem = '56g'\n",
    "\n",
    "\n",
    "\n",
    "Job_Name = 'Dev Nandan : Customer_Address'\n",
    "\n",
    "\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql.functions import asc,lit\n",
    "#warnings.filterwarnings('error')\n",
    "import pyspark\n",
    "from datetime import datetime,timedelta\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "#import numpy\n",
    "import calendar\n",
    "#import pandas as pd\n",
    "#import simplejson as json\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import lit\n",
    "import simplejson as json\n",
    "import json, pprint, requests\n",
    "#es_nodes = '10.35.12.5'\n",
    "#es_nodes = '10.35.12.6'\n",
    "es_nodes = '10.35.12.194'#,10.35.12.6,10.35.12.5\n",
    "es_nodes_temp='10.35.12.194'\n",
    "es_port = '5432'\n",
    "es_user = 'gpanalytics'\n",
    "es_pwd = ''\n",
    "mesos_ip = 'mesos://10.33.195.18:5050'#'mesos://10.35.12.5:5050'\n",
    "\n",
    "spark.stop() #############NEED TO COMMENT THIS SPARK.STOP WHEN RUNNING THROUGH SHELL###############################\n",
    "\n",
    "conf.setMaster(mesos_ip)\n",
    "\n",
    "conf.set('spark.executor.cores',no_of_cpu) ### change 1\n",
    "#conf.set('spark.memory.fraction','.2')\n",
    "conf.set('spark.executor.memory',executor_mem) \n",
    "conf.set('spark.es.scroll.size','10000')\n",
    "conf.set('spark.network.timeout','600s')\n",
    "conf.set('spark.sql.crossJoin.enabled', 'true')\n",
    "\n",
    "conf.set('spark.ui.port','4052')\n",
    "\n",
    "conf.set('spark.executor.heartbeatInterval','60s')\n",
    "conf.set(\"spark.driver.cores\",\"4\")\n",
    "conf.set(\"spark.driver.extraJavaOptions\",\"-Xmx4g -Xms4g\")\n",
    "\n",
    "#conf.set(\"spark.shuffle.blockTransferService\", \"nio\"); \n",
    "conf.set(\"spark.files.overwrite\",\"true\");\n",
    "conf.set(\"spark.kryoserializer.buffer\", \"70\"); \n",
    "conf.set(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\");\n",
    "conf.set(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\");\n",
    "conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\"); \n",
    "conf.set(\"spark.broadcast.compress\", \"true\"); \n",
    "conf.set(\"spark.shuffle.compress\", \"true\"); \n",
    "conf.set(\"spark.shuffle.spill.compress\", \"true\");\n",
    "conf.set(\"spark.app.name\", Job_Name)\n",
    "#conf.set(\"spark.io.compression.codec\",\"org.apache.spark.io.LZ4CompressionCodec\");\n",
    "#conf.set(\"spark.sql.inMemoryColumnarStorage.compressed\", \"true\"); \n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "conf.set('spark.driver.memory','20g') ### change 2\n",
    "conf.set('spark.cores.max',max_cores) ### change 3\n",
    "conf.set('spark.sql.shuffle.partitions','400')\n",
    "#conf.set('spark.sql.crossJoin.enabled', 'true')\n",
    "conf.set('es.nodes',es_nodes)\n",
    "conf.set('es.port',es_port)\n",
    "conf.set('es.nodes.wan.only','true')\n",
    "conf.set(\"spark.sql.autoBroadcastJoinThreshold\",-1)\n",
    "\n",
    "#conf.set('spark.es.net.http.auth.user','Spark')\n",
    "#conf.set('spark.es.net.http.auth.pass','Jarkpet1Sap3')\n",
    "conf.set('spark.num.executors','4')\n",
    "conf.set('spark.debug.maxToStringFields', 200)\n",
    "\n",
    "conf.set('spark.es.net.http.auth.user', es_user)\n",
    "conf.set('spark.es.net.http.auth.pass', es_pwd)\n",
    "\n",
    "conf.set('spark.es.mapping.date.rich','false')\n",
    "spark = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(spark)\n",
    "\n",
    "# Load Data into PySpark DataFrames\n",
    "# Prodcom Data Frame\n",
    "import json, pprint, requests\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hash\n",
    "import numpy as np\n",
    "import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import DateType\n",
    "import psycopg2\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import pytz\n",
    "starttime = time.time()\n",
    "start_time = datetime.datetime.now(pytz.timezone('Asia/Kolkata')).strftime(\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prod_url = \"jdbc:postgresql://10.35.12.194:5432/gpadmin\"\n",
    "prod_host = '10.35.12.194'\n",
    "prod_port = '5432'\n",
    "prod_dbname = 'gpadmin'\n",
    "user_prod=\"gpspark\"\n",
    "pwd_prod=\"spark@456\"\n",
    "dbschema=\"public\"\n",
    "\n",
    "\n",
    "prod_gpdb_spark_options ={\n",
    "    \"url\": \"jdbc:postgresql://{host}:{port}/{dbname}\".format(host=prod_host,port=prod_port, dbname=prod_dbname),\n",
    "    \"user\": \"{user}\".format(user=user_prod),\n",
    "    \"password\": \"{password}\".format(password=pwd_prod)\n",
    "} \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gpdb_jdbc(col_str,dbtable,col_name=None,time_filter=None,partitionColumn=\"row_num\"):\n",
    "    \"\"\"\n",
    "    This is used to read gpdb with filter for columns and can apply other filter(date,values).\n",
    "    Time filter contains startdate,enddate\n",
    "    \"\"\"\n",
    "    gscPythonOptions = {\n",
    "                        \"url\": prod_url,\n",
    "                        \"user\": user_prod,\n",
    "                        \"password\": pwd_prod,\n",
    "                        \"dbschema\": dbschema,\n",
    "                        \"dbtable\": dbtable,\n",
    "                        \"partitionColumn\":partitionColumn,\n",
    "                        \"partitions\": 8,\n",
    "                        \"server.port\":\"1150-1170\"}\n",
    "   \n",
    "    if time_filter:\n",
    "        data = sqlContext.read.format(\"greenplum\").options(**gscPythonOptions).load()\\\n",
    "                .selectExpr(col_str).drop_duplicates().filter(col(col_name).between(to_timestamp(lit(time_filter['start_date']),\n",
    "                                                                 format='yyyy-MM-dd'),\n",
    "                                                    to_timestamp(lit(time_filter['end_date']),\n",
    "                                                                 format='yyyy-MM-dd')))\n",
    "    else :\n",
    "        data = sqlContext.read.format(\"greenplum\").options(**gscPythonOptions).load()\\\n",
    "                .selectExpr(col_str).drop_duplicates()\n",
    "   \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_progress(table_name):\n",
    "    \n",
    "    try: \n",
    "        gscPythonOptions = {\n",
    "                 \"url\":prod_url ,\n",
    "                 \"user\":user_prod ,\n",
    "                 \"password\": pwd_prod,\n",
    "                 \"dbschema\":\"customermart\",\n",
    "                 \"dbtable\": \"progress\",#table change\n",
    "                 \"server.port\":\"1150-1170\"} \n",
    "        \n",
    "        # this query will fetch till what date we have inserted the records in target\n",
    "        last_run= sqlContext.read.format(\"greenplum\").options(**gscPythonOptions).load()\\\n",
    "            .select('table_name','source','to_datetime').filter(col('table_name')==table_name)\\\n",
    "            .filter(col('source')=='home').filter(col('records')!='0')\n",
    "        \n",
    "        Max_last_run =last_run.select(max('to_datetime')).first()[0]\n",
    "\n",
    "\n",
    "        # if no record is available in progress then it will raise value error, in this case default value will be picked from except clause\n",
    "        if Max_last_run is None:\n",
    "            raise ValueError(\"No records found!\")\n",
    "\n",
    "        #print(\"Record Found. Progress updated till {}\".format(Max_last_run))\n",
    "\n",
    "    except Exception as E: \n",
    "        Max_last_run = datetime.datetime(2018, 10, 31, 0, 0)  # this is the default start date when no record is present in progress for customer_demographics table\n",
    "        print(\"Executed_except\",repr(E))\n",
    "    return Max_last_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic function to save the progress from last run into Progress table\n",
    "def update_progress(table_name,source,time_filter,records,start_time,starttime,status):\n",
    "    output_index = \"progress\"\n",
    "    schema = \"customermart\"\n",
    "\n",
    "\n",
    "    import sys\n",
    "    import time\n",
    "    try:\n",
    "        \n",
    "                \n",
    "#         df_progress_values = [{\"table_name\":table_name,\"source\":source,\"from_datetime\":time_filter['start_date'].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "#                         ,\"to_datetime\":time_filter['end_date'].strftime(\"%Y-%m-%d %H:%M:%S\"),'records':records,\n",
    "#                          'start_time':start_time}]\n",
    "\n",
    "        df_progress= sqlContext.createDataFrame([(table_name,source,\n",
    "                                       time_filter['start_date'].strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                                         time_filter['end_date'].strftime(\"%Y-%m-%d %H:%M:%S\"),records)]\n",
    "                                     ,['table_name', 'source','from_datetime','to_datetime','records'])\\\n",
    "        .withColumn('start_time',lit(start_time)).withColumn('end_time',current_timestamp())\\\n",
    "                    .withColumn('executed_in_mins',lit((time.time() - starttime)/60))\\\n",
    "                                .withColumn('status',lit(status)).withColumn('remarks',lit(None))\n",
    "        \n",
    "#         schema = StructType([StructField(\"table_name\", StringType(),True),StructField(\"source\", StringType(),True),\\\n",
    "#                      StructField(\"from_datetime\", StringType(),True),StructField(\"to_datetime\", StringType(),True),\\\n",
    "#                      StructField(\"records\", IntegerType(),True),StructField(\"start_time\", StringType(),True)])\n",
    "\n",
    "#         df_progress = sqlContext.createDataFrame(df_progress_values,schema)\n",
    "\n",
    "        \n",
    "        df_progress.write.format(\"greenplum\")\\\n",
    "            .option(\"dbtable\",output_index).option('dbschema',schema)\\\n",
    "            .option(\"server.port\",\"1150-1170\").option(\"url\",prod_url)\\\n",
    "            .option(\"user\", user_prod).option(\"password\",pwd_prod).mode('append').save()\n",
    "        \n",
    "       \n",
    "    except Exception as e:\n",
    "        x = e\n",
    "        print(x)\n",
    "    else:\n",
    "        x = 'success'\n",
    "        print(\"Updated Progress for {table_name} from {source} until {to_datetime}. {records} records processed in this run\".format(table_name=table_name, source=source,to_datetime=time_filter['end_date'].strftime(\"%Y-%m-%d %H:%M:%S\"),records=records))\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table_dict={\n",
    "    \"address\" : #source/subsegment\n",
    "              {\n",
    "                  \"out_columns\" : ['source_system_name', 'source_system_customer_id','activity_id','activity_date',\n",
    "                                   'address_type',\n",
    "                                   'address_id','address_line_1','address_line_2','address_line_3','area','city',\n",
    "                                   'district','state',\n",
    "                                   'country','pin_code','landmark','verification_flag', 'verification_source', 'last_updated'],\n",
    "                  \"primary_key\" : ['source_system_name','source_system_customer_id','activity_id'],\n",
    "                  \"date_sort_column\": \"last_updated\",\n",
    "                  \"home\":\n",
    "                     {\n",
    "                          \"schema\":\"public\",\n",
    "                          \"table\": \"mastercraft_bulk_detail_prod\",                         \n",
    "                          \"update_fields\" : ['verification_flag', 'verification_source', 'last_updated'],\n",
    "                          \"in_columns\" : ['source_id','source_customer_id','dc_unified_id','dc_action_date','psnt_address_line_1','psnt_address_line_2','psnt_address_line_3','psnt_address_city','psnt_address_state','psnt_address_country','psnt_address_zip','psnt_address_landmark'],\n",
    "                          \"required_columns\" : ['dc_unified_id','source_id'],\n",
    "                          \"date_column\" : \"dc_action_date\",\n",
    "                          \"transform_expr\":[\n",
    "                                \"coalesce(source_customer_id,dc_unified_id) as source_system_customer_id\",\n",
    "                                \"source_id as source_system_name\",\n",
    "                                \"hash(concat_ws(' ',psnt_address_line_1,psnt_address_line_2,psnt_address_line_3,psnt_address_city,psnt_address_state,psnt_address_country,psnt_address_zip,psnt_address_landmark)) as activity_id\",\n",
    "                                \"dc_action_date as activity_date\",\n",
    "                                \"'home' as address_type\",\n",
    "                                \"hash(concat_ws(' ',psnt_address_line_1,psnt_address_line_2,psnt_address_line_3,psnt_address_city,psnt_address_country,psnt_address_zip,psnt_address_landmark)) as address_id\",\n",
    "                                \"psnt_address_line_1 as address_line_1\",\n",
    "                                \"psnt_address_line_2 as address_line_2\",\n",
    "                                \"psnt_address_line_3 as address_line_3\",\n",
    "                                \"null as area\",\n",
    "                                \"psnt_address_city as city\",\n",
    "                                \"psnt_address_dist as district\",\n",
    "                                \"psnt_address_state as state\",\n",
    "                                \"psnt_address_country as country\",\n",
    "                                \"psnt_address_zip as pin_code\",\n",
    "                                \"psnt_address_landmark as landmark\",\n",
    "                                \"case when psnt_address_is_verified is null then 'UV' else 'IV' end as verification_flag\",\n",
    "                                \"case when psnt_address_is_verified is null then null else 'Mastercraft' end as verification_source\",\n",
    "                                \"dc_action_date as last_updated\",\n",
    "                                \"null as address_json\"] \n",
    "                     },\n",
    "                \"communication\": #source/subsegment\n",
    "                     {\n",
    "                          \"schema\":\"public\",\n",
    "                          \"table\": \"mastercraft_bulk_detail_prod\",                         \n",
    "                          \"update_fields\" : ['verification_flag', 'verification_source', 'last_updated'],\n",
    "                          \"in_columns\" : ['source_id','source_customer_id','dc_unified_id','dc_action_date','comm_address_line_1','comm_address_line_2',\n",
    "                                         'comm_address_line_3','comm_address_city','comm_address_dist','comm_address_state',\n",
    "                                         'comm_address_country','comm_address_zip','comm_address_landmark','comm_address_is_verified'],\n",
    "                          \"required_columns\" : ['dc_unified_id','source_id'],\n",
    "                          \"date_column\" : \"dc_action_date\",\n",
    "                          \"transform_expr\":[\n",
    "                                \"coalesce(source_customer_id,dc_unified_id) as source_system_customer_id\",\n",
    "                                \"source_id as source_system_name\",\n",
    "                                \"hash(concat_ws(' ',comm_address_line_1,comm_address_line_2,comm_address_line_3,comm_address_city,comm_address_dist,comm_address_state,comm_address_country,comm_address_zip,comm_address_landmark)) as activity_id\",\n",
    "                                \"dc_action_date as activity_date\",\n",
    "                                \"'communication' as address_type\",\n",
    "                                \"hash(concat_ws(' ',comm_address_line_1,comm_address_line_2,comm_address_line_3,comm_address_city,comm_address_dist,comm_address_state,comm_address_country,comm_address_zip,comm_address_landmark)) as address_id\",\n",
    "                                \"comm_address_line_1 as address_line_1\",\n",
    "                                \"comm_address_line_2 as address_line_2\",\n",
    "                                \"comm_address_line_3 as address_line_3\",\n",
    "                                \"null as area\",\n",
    "                                \"comm_address_city as city\",\n",
    "                                \"comm_address_dist as district\",\n",
    "                                \"comm_address_state as state\",\n",
    "                                \"comm_address_country as country\",\n",
    "                                \"comm_address_zip as pin_code\",\n",
    "                                \"comm_address_landmark as landmark\",\n",
    "                                \"case when comm_address_is_verified is null then 'UV' else 'IV' end as verification_flag\",\n",
    "                                \"case when comm_address_is_verified is null then null else 'Mastercraft' end as verification_source\",\n",
    "                                \"dc_action_date as last_updated\",\n",
    "                                \"null as address_json\"] \n",
    "                     },\n",
    "                \"permanent\": #source/subsegment\n",
    "                     {\n",
    "                          \"schema\":\"public\",\n",
    "                          \"table\": \"mastercraft_bulk_detail_prod\",                         \n",
    "                          \"update_fields\" : ['verification_flag', 'verification_source', 'last_updated'],\n",
    "                          \"in_columns\" : ['source_id','source_customer_id','dc_unified_id','dc_action_date','pmt_address_line_1','pmt_address_line_2',\n",
    "                                         'pmt_address_line_3','pmt_address_city','pmt_address_dist','pmt_address_state',\n",
    "                                         'pmt_address_country','pmt_address_zip','pmt_address_landmark','pmt_address_is_verified'],\n",
    "                          \"required_columns\" : ['dc_unified_id','source_id'],\n",
    "                          \"date_column\" : \"dc_action_date\",\n",
    "                          \"transform_expr\":[\n",
    "                                \"coalesce(source_customer_id,dc_unified_id) as source_system_customer_id\",\n",
    "                                \"source_id as source_system_name\",\n",
    "                                \"hash(concat_ws(' ',pmt_address_line_1,pmt_address_line_2,pmt_address_line_3,pmt_address_city,pmt_address_dist,pmt_address_state,pmt_address_country,pmt_address_zip,pmt_address_landmark)) as activity_id\",\n",
    "                                \"dc_action_date as activity_date\",\n",
    "                                \"'permanent' as address_type\",\n",
    "                                \"hash(concat_ws(' ',pmt_address_line_1,pmt_address_line_2,pmt_address_line_3,pmt_address_city,pmt_address_dist,pmt_address_state,pmt_address_country,pmt_address_zip,pmt_address_landmark)) as address_id\",\n",
    "                                \"pmt_address_line_1 as address_line_1\",\n",
    "                                \"pmt_address_line_2 as address_line_2\",\n",
    "                                \"pmt_address_line_3 as address_line_3\",\n",
    "                                \"null as area\",\n",
    "                                \"pmt_address_city as city\",\n",
    "                                \"pmt_address_dist as district\",\n",
    "                                \"pmt_address_state as state\",\n",
    "                                \"pmt_address_country as country\",\n",
    "                                \"pmt_address_zip as pin_code\",\n",
    "                                \"pmt_address_landmark as landmark\",\n",
    "                                \"case when pmt_address_is_verified is null then 'UV' else 'IV' end as verification_flag\",\n",
    "                                \"case when pmt_address_is_verified is null then null else 'Mastercraft' end as verification_source\",\n",
    "                                \"dc_action_date as last_updated\",\n",
    "                                \"null as address_json\"] \n",
    "                     }\n",
    "              }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(spark_df,table_name, source, table_dict, prod_gpdb_spark_options, schema = \"customermart\"):\n",
    "    from pyspark.sql import functions as F\n",
    "    transform_expr = table_dict.get(table_name).get(source).get(\"transform_expr\")\n",
    "    if transform_expr == None:\n",
    "        print(\"No Transformation found for this combination - {table_name},{source}\".\n",
    "              format(table_name=table_name,source=source))\n",
    "        return None\n",
    "    primary_key = table_dict.get(table_name).get(\"primary_key\")\n",
    "    date_sort_column = table_dict.get(table_name).get(\"date_sort_column\")\n",
    "    if (primary_key == None) | (date_sort_column == None) :\n",
    "        print(\"No Primary Key OR Sort Key found for this table - {}\".format(table_name))\n",
    "        return None\n",
    "    \n",
    "    # each target table has a temp table with name table_name1\n",
    "    # picking data from temp table (to get column names and their schema)\n",
    "    \n",
    "    df_1 = sqlContext.read.format(\"greenplum\").options(**prod_gpdb_spark_options).option(\"dbschema\",schema)\\\n",
    "    .option(\"dbtable\",\"{}_staging\".format(table_name)).load()\n",
    "    dfschema = df_1.schema \n",
    "    \n",
    "    from pyspark.sql import functions as F\n",
    "    \n",
    "    # applying transformations on spark_df\n",
    "    spark_df = spark_df.selectExpr(transform_expr).select(*df_1.columns)\n",
    "    final_df = sqlContext.createDataFrame(spark_df.rdd,dfschema).orderBy(F.desc(\"{}\".format(date_sort_column))).drop_duplicates(primary_key).cache()\n",
    "    records = final_df.count()\n",
    "#     print(gpdb_spark_options)\n",
    "    \n",
    "#     inserting the data into temp table\n",
    "    output_index = \"address_staging\"#customer_demographics_naddenk_01_07_22\n",
    "    schema = \"customermart\"\n",
    "\n",
    "\n",
    "    import sys\n",
    "    try:\n",
    "\n",
    "        final_df.write.option(\"truncate\", \"true\").format(\"greenplum\")\\\n",
    "        .option(\"dbtable\",output_index).option('dbschema',schema)\\\n",
    "        .option(\"server.port\",\"1150-1170\").option(\"url\",prod_url)\\\n",
    "        .option(\"user\", user_prod).option(\"password\",pwd_prod).mode('overwrite').save()\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        x = e\n",
    "        print(x)\n",
    "    else:\n",
    "        x = 200 #success\n",
    "        print(x)\n",
    "    return records\n",
    "    #     print(\"{} records successfully Inserted in {}_staging!\".format(records,table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generic update function which takes the records currently in temp table (created with prefix 1 to original customercoe table)\n",
    "# This temp table has similar schema as original table but no json field and would not contain duplicates\n",
    "# The function would first try to insert the records in original table. It that fails then it will upsert the records\n",
    "# Since there is no direct upsert query an update and insert query with where clause is used\n",
    "# This funciton just runs the SQL queries in GPDB and does not use spark\n",
    "\n",
    "def update(conn_to,table_name,source,gpdb_spark_options = prod_gpdb_spark_options,\n",
    "                  schema=\"customermart\"):\n",
    "    conn_to.rollback()\n",
    "    update_fields = table_dict.get(table_name).get(source).get(\"update_fields\")\n",
    "    primary_key = table_dict.get(table_name).get(\"primary_key\")\n",
    "    \n",
    "    # loading data from temp table to get column names\n",
    "    df_1 = sqlContext.read.format(\"greenplum\").options(**gpdb_spark_options).option(\"dbschema\",schema)\\\n",
    "    .option(\"dbtable\",\"{}_staging\".format(table_name)).load()\n",
    "    out_columns = df_1.columns\n",
    "    out_columns = [col for col in out_columns if \"json\" not in col ]\n",
    "    #out_columns = table_dict.get(table_name).get(\"out_columns\")\n",
    "#     records=df_1.count()\n",
    "\n",
    "    cur_to = conn_to.cursor()  \n",
    "\n",
    "    # this query insert records from temp table to target table when all the records in temp table are new and not present in target\n",
    "    only_insert_query = \"\"\"insert into {schema}.{table_name} ({out_columns}) select {out_columns} from {schema}.{table_name}_staging \"\"\".format(schema=schema,table_name=table_name,out_columns = \",\".join(out_columns))\n",
    "    \n",
    "    # this query will be executed to update the records that are already present in target, new values will be assigned from temp table\n",
    "    update_query = \"\"\"UPDATE {schema}.{table_name} orig\n",
    "                          SET\n",
    "                            {update_fields}\n",
    "                          FROM\n",
    "                            {schema}.{table_name}_staging temp\n",
    "                           WHERE \n",
    "                            {primary_key}\n",
    "                       \"\"\".format(schema=schema,table_name=table_name,primary_key= \" and \".join([ \"orig.{key} = temp.{key}\".format(key=key) for key in primary_key]),update_fields = \" , \".join([ \"{key} = temp.{key}\".format(key=key) for key in update_fields]))\n",
    "    \n",
    "    # this query will be executed after update query to insert the remaining records\n",
    "    insert_query = \"\"\" INSERT INTO {schema}.{table_name} ({out_columns})\n",
    "                           SELECT {out_columns}\n",
    "                           FROM\n",
    "                             {schema}.{table_name}_staging temp\n",
    "                           WHERE\n",
    "                             NOT EXISTS (\n",
    "                             SELECT 1 FROM {schema}.{table_name} orig WHERE \n",
    "                            {primary_key}\n",
    "                            )\n",
    "                        \"\"\".format(schema=schema,table_name=table_name,out_columns = \",\".join(out_columns),primary_key = \" and \".join([ \"orig.{key} = temp.{key}\".format(key=key) for key in primary_key]))\n",
    "\n",
    "    try:\n",
    "        print(\"Inside try segment\")\n",
    "        cur_to.execute(only_insert_query)\n",
    "        print(\"Executed Only insert query\")\n",
    "        #update_progress(table_name,source,time_filter,records)\n",
    "    except Exception as e:\n",
    "        print(\"Inside except segment\")\n",
    "        conn_to.rollback()\n",
    "        cur_to.execute(update_query)\n",
    "        print(\"Executed update query\")\n",
    "        cur_to.execute(insert_query)\n",
    "        print(\"Executed insert query\")\n",
    "        #update_progress(table_name,source,time_filter,records)\n",
    "    conn_to.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"address\"\n",
    "source = \"permanent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_progress=get_latest_progress(table_name)\n",
    "# dates in string format\n",
    "str_d1 = latest_progress.strftime(\"%Y-%m-%d\")\n",
    "str_d2 = datetime.datetime.now(pytz.timezone('Asia/Kolkata')).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# convert string to date object\n",
    "d1 = datetime.datetime.strptime(str_d1, \"%Y-%m-%d\")\n",
    "d2 = datetime.datetime.strptime(str_d2, \"%Y-%m-%d\")\n",
    "\n",
    "# difference between dates in timedelta\n",
    "delta = d2 - d1\n",
    "day=delta.days-1  #currently T-2 change 2 to 1 for T-1\n",
    "\n",
    "time_filter={'start_date':latest_progress,'end_date':latest_progress+datetime.timedelta(days=day)}\n",
    "time_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "col_str=['source_id','source_customer_id','dc_unified_id','psnt_address_line_1','psnt_address_line_2','psnt_address_line_3',\n",
    "         'coalesce(pstincomingcity,psnt_address_city) as psnt_address_city',\n",
    "         'coalesce(pstincomingstate,psnt_address_state) as psnt_address_state','psnt_address_country',\n",
    "         'coalesce(pstincomingpin,psnt_address_zip) as psnt_address_zip','psnt_address_dist',\n",
    "         'psnt_address_landmark','psnt_address_is_verified',\n",
    "         'comm_address_line_1','comm_address_line_2','comm_address_line_3','coalesce(cmnincomingcity,comm_address_city) as comm_address_city' ,\n",
    "         'comm_address_dist',\n",
    "         'coalesce(cmnincomingstate,comm_address_state) as comm_address_state','comm_address_country',\n",
    "         'coalesce(cmnincomingpin,comm_address_zip) as comm_address_zip','comm_address_landmark','comm_address_is_verified',\n",
    "         'pmt_address_line_1','pmt_address_line_2','pmt_address_line_3','coalesce(pmtincomingcity,pmt_address_city) as pmt_address_city',\n",
    "         'pmt_address_dist','coalesce(pmtincomingstate,pmt_address_state) as pmt_address_state',\n",
    "         'pmt_address_country','coalesce(pmnincomingpin,pmt_address_zip) as pmt_address_zip',\n",
    "         'pmt_address_landmark','pmt_address_is_verified', 'dc_action_date']\n",
    "\n",
    "master_bulk_df=load_gpdb_jdbc(col_str,\"mastercraft_bulk_detail_prod\",\"dc_action_date\",time_filter)\n",
    "spark_df=master_bulk_df.filter((col('dc_unified_id').isNotNull()) & (col('source_id').isNotNull())) \n",
    "\n",
    "spark_df = spark_df.withColumn('psnt_address_zip',when(col('psnt_address_zip').rlike(\"^[1-9]{1}[0-9]{2}\\s{0,1}[0-9]{3}$\"),\\\n",
    "                                                       col('psnt_address_zip')).otherwise(lit(None)))\n",
    "\n",
    "spark_df = spark_df.withColumn('comm_address_zip',when(col('comm_address_zip').rlike(\"^[1-9]{1}[0-9]{2}\\s{0,1}[0-9]{3}$\"),\\\n",
    "                                                       col('comm_address_zip')).otherwise(lit(None)))\n",
    "\n",
    "spark_df = spark_df.withColumn('pmt_address_zip',when(col('pmt_address_zip').rlike(\"^[1-9]{1}[0-9]{2}\\s{0,1}[0-9]{3}$\"),\\\n",
    "                                                       col('pmt_address_zip')).otherwise(lit(None)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.persist(pyspark.StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if source ==\"permanent\":\n",
    "\n",
    "    df_per=spark_df.filter((col('pmt_address_city').isNotNull()) | (col('pmt_address_state').isNotNull()) | (col('pmt_address_zip').isNotNull()))\n",
    "    if df_per.count()>0:\n",
    "        records=prep_data(df_per,\\\n",
    "                  table_name,\\\n",
    "                  source,\\\n",
    "                  table_dict,\\\n",
    "                  prod_gpdb_spark_options\n",
    "                 )\n",
    "        conn_prod = psycopg2.connect(host=prod_host, port=prod_port, user=user_prod, password=pwd_prod, dbname=prod_dbname)\n",
    "        conn_to=conn_prod\n",
    "        import sys\n",
    "        try:\n",
    "            update(conn_to,table_name,source,prod_gpdb_spark_options, schema=\"customermart\")\n",
    "        except Exception as e:\n",
    "            x = e\n",
    "            print(x)\n",
    "        else:\n",
    "            x = 'success'\n",
    "            print(x)\n",
    "        if  x == 'success':\n",
    "            update_progress(table_name,source,time_filter,records,start_time,starttime,'Success')\n",
    "            print('SUCCESS')\n",
    "        else:\n",
    "            update_progress(table_name,source,time_filter,'0',start_time,starttime,'Failed')\n",
    "            print('FAILED')\n",
    "        conn_prod.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"address\"\n",
    "source = \"home\"\n",
    "conn_prod = psycopg2.connect(host=prod_host, port=prod_port, user=user_prod, password=pwd_prod, dbname=prod_dbname)\n",
    "conn_to=conn_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if source ==\"home\":\n",
    "    df_home=spark_df.filter((col('psnt_address_city').isNotNull()) | (col('psnt_address_state').isNotNull()) | (col('psnt_address_zip').isNotNull()))\n",
    "    if df_home.count()>0:\n",
    "        records=prep_data(df_home,\\\n",
    "                  table_name,\\\n",
    "                  source,\\\n",
    "                  table_dict,\\\n",
    "                  prod_gpdb_spark_options\n",
    "                 )\n",
    "\n",
    "        conn_prod = psycopg2.connect(host=prod_host, port=prod_port, user=user_prod, password=pwd_prod, dbname=prod_dbname)\n",
    "        conn_to=conn_prod\n",
    "        import sys\n",
    "        try:\n",
    "            update(conn_to,table_name,source,prod_gpdb_spark_options, schema=\"customermart\")\n",
    "        except Exception as e:\n",
    "            x = e\n",
    "            print(x)\n",
    "        else:\n",
    "            x = 'success'\n",
    "            print(x)\n",
    "        if  x == 'success':\n",
    "            update_progress(table_name,source,time_filter,records,start_time,starttime,'Success')\n",
    "            print('SUCCESS')\n",
    "        else:\n",
    "            update_progress(table_name,source,time_filter,'0',start_time,starttime,'Failed')\n",
    "            print('FAILED')\n",
    "        conn_prod.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"address\"\n",
    "source = \"communication\"\n",
    "conn_prod = psycopg2.connect(host=prod_host, port=prod_port, user=user_prod, password=pwd_prod, dbname=prod_dbname)\n",
    "conn_to=conn_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if source ==\"communication\":\n",
    "    df_com=spark_df.filter((col('comm_address_city').isNotNull()) | (col('comm_address_state').isNotNull()) | (col('comm_address_zip').isNotNull()))\n",
    "    if df_com.count()>0:\n",
    "        print(df_com.count())\n",
    "        records=prep_data(df_com,\\\n",
    "                  table_name,\\\n",
    "                  source,\\\n",
    "                  table_dict,\\\n",
    "                  prod_gpdb_spark_options\n",
    "                 )\n",
    "        conn_prod = psycopg2.connect(host=prod_host, port=prod_port, user=user_prod, password=pwd_prod, dbname=prod_dbname)\n",
    "        conn_to=conn_prod\n",
    "        import sys\n",
    "        try:\n",
    "            update(conn_to,table_name,source,prod_gpdb_spark_options, schema=\"customermart\")\n",
    "        except Exception as e:\n",
    "            x = e\n",
    "            print(x)\n",
    "        else:\n",
    "            x = 'success'\n",
    "            print(x)\n",
    "        if  x == 'success':\n",
    "            update_progress(table_name,source,time_filter,records,start_time,starttime,'Success')\n",
    "            print('SUCCESS')\n",
    "        else:\n",
    "            update_progress(table_name,source,time_filter,'0',start_time,starttime,'Failed')\n",
    "            print('FAILED')\n",
    "        conn_prod.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
